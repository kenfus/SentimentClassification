{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "**Sentiment Analysis of IMDB Movie Reviews**\n",
    "\n",
    "\n",
    "This Notebook is heavily based on the Notebook by [Lakshmipathi N](https://www.kaggle.com/lakshmi25npathi) found on [Kaggle](https://www.kaggle.com/lakshmi25npathi/sentiment-analysis-of-imdb-movie-reviews)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1424638f5259100af9f9a5c1b05bd23cf5b71e51"
   },
   "source": [
    "**Import necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#Load the libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from scipy.sparse import hstack\n",
    "from collections import Counter\n",
    "# https://online.stat.psu.edu/stat504/lesson/1/1.7\n",
    "from utils import preprocesser_text, binarize_sentiment, train_test_split, evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "be1b642cce343f7a8f68f8c91f7c50372cdf4381"
   },
   "source": [
    "**Import the training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "4c593c17588723c0b0b0f19851cb70a8447ced76",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing the training data\n",
    "imdb_data=pd.read_csv('data/IMDB Dataset.csv')\n",
    "print(imdb_data.shape)\n",
    "imdb_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1ad3773974351ed9bdf389b2847d7475b36c2295"
   },
   "source": [
    "**Exploratery data analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "7f11c83b1320c8982b36889145f7f770563674a8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>49582</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review sentiment\n",
       "count                                               50000     50000\n",
       "unique                                              49582         2\n",
       "top     Loved today's show!!! It was a variety and not...  positive\n",
       "freq                                                    5     25000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Summary of the dataset\n",
    "imdb_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "453c3fd238f62ab8f649eb01771817e25bc0c77d"
   },
   "source": [
    "**Sentiment count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "cb6bb97b0f851947dcf341a1de5708a1f2bc64c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    25000\n",
       "negative    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentiment count\n",
    "imdb_data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset is perfectly balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f61964573faababe1f7897b77d32815a24954d2f"
   },
   "source": [
    "**Text normalization**\n",
    "\n",
    "The following text-normalization are applied:\n",
    "\n",
    "1.\tremoving the HTML tags. These are not relevant for understanding the input.\n",
    "2.\tremoving brackets with text (it seems that the text in brackets was either always a hyperlink or quote or reference, which are not too useful for the sentiment analysis).\n",
    "3.\tremoving special characters, e. g. ?,!,/\n",
    "4.\tremoving stop words. Stop words are common or “filler” words, which contain little information. For example, connection words. They are removed.\n",
    "5.\tput words into the basic form with the Porter-Stemmer-Algorithm, which applies multiple, hardcoded rules to reduce the word-length. Some examples: `likes`, `liked`, `likely` and `liking` will all be reduced to `like`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 50000/50000 [00:08<00:00, 5806.83it/s]\n",
      "Pandas Apply: 100%|██████████| 50000/50000 [00:00<00:00, 373110.92it/s]\n",
      "Pandas Apply: 100%|██████████| 50000/50000 [00:01<00:00, 43077.99it/s]\n",
      "Pandas Apply: 100%|██████████| 50000/50000 [02:54<00:00, 287.17it/s]\n",
      "Pandas Apply: 100%|██████████| 50000/50000 [00:46<00:00, 1064.45it/s]\n"
     ]
    }
   ],
   "source": [
    "imdb_data_norm = preprocesser_text(imdb_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "90da29c3b79f46f41d7391a2a116065b616d0fac"
   },
   "source": [
    "**Train Test Split**\n",
    "\n",
    "When doing a very basic train/test-split, like we are doing here with taking the first 40000 as train and the last 10000 as test, we should be sure that we have close to a 50/50 Balance of Classes in the Train-Test set. This is the case here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "b20c242bd091929ca896ea2c6e936ca00efe6ecf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative    20007\n",
      "positive    19993\n",
      "Name: sentiment, dtype: int64\n",
      "positive    5007\n",
      "negative    4993\n",
      "Name: sentiment, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'one review ha mention watch 1 oz episod youll hook right thi exactli happen meth first thing struck oz wa brutal unflinch scene violenc set right word go trust thi show faint heart timid thi show pull punch regard drug sex violenc hardcor classic use wordit call oz nicknam given oswald maximum secur state penitentari focus mainli emerald citi experiment section prison cell glass front face inward privaci high agenda em citi home manyaryan muslim gangsta latino christian italian irish moreso scuffl death stare dodgi deal shadi agreement never far awayi would say main appeal show due fact goe show wouldnt dare forget pretti pictur paint mainstream audienc forget charm forget romanceoz doesnt mess around first episod ever saw struck nasti wa surreal couldnt say wa readi watch develop tast oz got accustom high level graphic violenc violenc injustic crook guard wholl sold nickel inmat wholl kill order get away well manner middl class inmat turn prison bitch due lack street skill prison experi watch oz may becom comfort uncomfort viewingthat get touch darker side'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalized train reviews\n",
    "norm_train, norm_test = train_test_split(imdb_data_norm)\n",
    "print(norm_train.sentiment.value_counts())\n",
    "print(norm_test.sentiment.value_counts())\n",
    "norm_train.review[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "52371868f05ff9cf157280c5acf0f5bc71ee176d"
   },
   "source": [
    "**Bags of words model**\n",
    "\n",
    "It is used to convert text documents to numerical vectors. It also creates n_grams, which basically means that if n = one, one word == 1 vector. If n == 2, a vector is made up of two neighbouring words. One row then is equal to how often a specific n_gram appears in the, in this case, Review. If we define an n_gram range, we union the new vectorized words and use them as feature.\n",
    "\n",
    "Sklearn also has two other interesting parameters:\n",
    "\n",
    "\n",
    "`min_df`: When using a float, it means that a word has to appear at least in x% documents. When using an int, it means that the word should appear at least in x documents.\n",
    "\n",
    "`max_df`: When using a float, it means that a word has to appear at most in x% documents. When using an int, it means that the word should appear at most in x documents.\n",
    "\n",
    "Below we will analyse the dimension of the newly generated features with Sklearns CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW_cv_train: (40000, 156136)\n",
      "BOW_cv_test: (10000, 156136)\n",
      "Vocab:  ['00', '000', '0000000000001', '00000001', '000001']\n",
      "Vocab Length:  156136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1x156136 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 143 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count vectorizer for bag of words and dimension analysis\n",
    "cv=CountVectorizer(ngram_range=(1,1), min_df=0, max_df=1.)\n",
    "#transformed train reviews\n",
    "cv_train_reviews=cv.fit_transform(norm_train.review)\n",
    "#transformed test reviews\n",
    "cv_test_reviews=cv.transform(norm_test.review)\n",
    "\n",
    "print('BOW_cv_train:',cv_train_reviews.shape)\n",
    "print('BOW_cv_test:',cv_test_reviews.shape)\n",
    "print('Some Vocab: ', cv.get_feature_names()[:5])\n",
    "print('Vocab Length: ', len(cv.get_feature_names()))\n",
    "cv_train_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000    first want say lean liber polit scale found mo...\n",
       "40001    wa excit see sitcom would hope repres indian c...\n",
       "40002    look cover read stuff entir differ type movi c...\n",
       "40003    like mani count appear denni hopper make thi c...\n",
       "40004    thi movi wa tv day didnt enjoy first georg jun...\n",
       "                               ...                        \n",
       "49995    thought thi movi right good job wasnt creativ ...\n",
       "49996    bad plot bad dialogu bad act idiot direct anno...\n",
       "49997    cathol taught parochi elementari school nun ta...\n",
       "49998    im go disagre previou comment side maltin thi ...\n",
       "49999    one expect star trek movi high art fan expect ...\n",
       "Name: review, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_test.review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dimension of the vectorized words does not match the number of unique words!\n"
     ]
    }
   ],
   "source": [
    "# We would expect that the dimension of the vectorized words would match the number of unique words.\n",
    "\n",
    "results = Counter()\n",
    "norm_train.review.str.split().apply(results.update)\n",
    "\n",
    "try:\n",
    "    assert len(results.keys()) ==  len(cv.get_feature_names()), \"The Dimension of the vectorized words does not match the number of unique words!\"\n",
    "except AssertionError as E:\n",
    "    print(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Words in CountVectorizer but not in the reviews:\n",
      "{'adventure', 'that', 'screams', 'in', 'settings', 'the', 'passed', 'mystery', 'and', 'oanyhow', 'actors'}\n",
      "Unique Words in the Dataset but not in the CountVectorizer:\n",
      "{'h', '^d', 'r', 'c', 'f^k', '^^', '6', '0', '_', '7', '2', 'e', '^^^', 'in\\\\al', 'spoilers^th', 'spoilers^^thi', 'time^^', '3', 'k', 'z', 'n', 'o^less', 'w\\\\', 'w', 'tortu^^^^^', '^', 'f', '5', 'f^', 'c^m', 'x', 'horror\\\\adventur', '1', '^^thi', '`', 'passed^mild', 'lives\\\\i', '\\\\the', 'screams\\\\jack', 'actors\\\\actress', 'hallmark\\\\lifetim', 'u', 'l', '^^contain', 'm^er', ']', '^_', '\\\\', 'horror\\\\fantasi', 'j', 'g', '^^i', '\\\\and\\\\', 'b', 'v', '10\\\\10i', '10^30', '^_^', '^oanyhow', '4', 'that^^', 'least^^', 'q', 'c^p', 'mexican\\\\english', '^____^the', 'hell^^', '5\\\\\\\\7', 'settings\\\\costum', '9', 'f^^ing', 'time^_^', 'p', '8', 'horror\\\\mystery\\\\action\\\\adventure\\\\combat'}\n"
     ]
    }
   ],
   "source": [
    "# However, this is not correct. Let see which one ar missing:\n",
    "print(\"Unique Words in CountVectorizer but not in the reviews:\")\n",
    "print(set(cv.get_feature_names()).difference(results.keys()))\n",
    "print(\"Unique Words in the Dataset but not in the CountVectorizer:\")\n",
    "print(set(results.keys()).difference(cv.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference is explainable by the regrex-pattern used by Sklearn CountVectorizer in the Parameter `token_patternstr`. It seems that our PreProcessing-Function did not reliably remove all special characters. Our Regex-Pattern removing special character should also be able to remove  `\\\\` and `^` but currently is not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "52371868f05ff9cf157280c5acf0f5bc71ee176d"
   },
   "source": [
    "**Term Frequency-Inverse Document Frequency model (TFIDF)**\n",
    "\n",
    "TF-IDF stands for Term Frequency - Inverse Document Frequency and is calculated for each word in the corpus. This value gives an insight into the importance of a term. The Term Frequency, i.e., the relative frequency of a term in a document combined with the relative, logarithmic frequency over all documents (formula 1) gives words that occur frequently and, in many documents, less weight than words that only occur in a few documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf_train: (40000, 6983231)\n",
      "Tfidf_test: (10000, 6983231)\n",
      "Tfidf_test: 6983231\n"
     ]
    }
   ],
   "source": [
    "#Tfidf vectorizer\n",
    "tv=TfidfVectorizer(ngram_range=(1,3), min_df=0., max_df=1.)\n",
    "#transformed train reviews\n",
    "tv_train_reviews=tv.fit_transform(norm_train.review)\n",
    "#transformed test reviews\n",
    "tv_test_reviews=tv.transform(norm_test.review)\n",
    "print('Tfidf_train:',tv_train_reviews.shape)\n",
    "print('Tfidf_test:',tv_test_reviews.shape)\n",
    "print('Tfidf_test:',len(tv.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "afe6de957339921e05a6faeaf731f2272fd31946"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf_train: (40000, 17053)\n",
      "Tfidf_test: (10000, 17053)\n",
      "Tfidf_test: 17053\n"
     ]
    }
   ],
   "source": [
    "#Tfidf vectorizer\n",
    "tv=TfidfVectorizer(ngram_range=(1,3), min_df=0.001, max_df=0.999)\n",
    "#transformed train reviews\n",
    "tv_train_reviews=tv.fit_transform(norm_train.review)\n",
    "#transformed test reviews\n",
    "tv_test_reviews=tv.transform(norm_test.review)\n",
    "print('Tfidf_train:',tv_train_reviews.shape)\n",
    "print('Tfidf_test:',tv_test_reviews.shape)\n",
    "print('Tfidf_test:',len(tv.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, if we filter out very common or very rare (with max_df and min_df), the vocab-size is reduced. In our example, from 6'983'231 to 17'053. As we will see later, this will improve our overall score by reducing overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "21a80c94fb42e14391c627710c5d796c40aa7dde"
   },
   "source": [
    "**Split and binarize the sentiment tdata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "ca1e4cc917265ac98a72c37cffe57f27e9897408"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "#Spliting the sentiment data\n",
    "train_sentiments = binarize_sentiment(norm_train.sentiment)\n",
    "test_sentiments = binarize_sentiment(norm_test.sentiment)\n",
    "print(train_sentiments.unique())\n",
    "print(test_sentiments.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelling the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d5e45fdc9d062a5b9b9dd665ffe732776e196953"
   },
   "source": [
    "To compare different parameters for the model and text preprocesser, we can create a function, which does all the preprocessing and modelling. This allows us to quickly compare and evaluate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_train_validate_evaluate(word_vectorizer, n_gram, analyzer, min_df, max_df, model, parameters, train_x, train_y, test_x, test_y, cv=5):\n",
    "    \"\"\"Pipeline to compare different types of preprocessing of words, models and parameters.\"\"\"\n",
    "    word_vectorizer = word_vectorizer(ngram_range=n_gram, analyzer=analyzer, min_df=min_df, max_df=max_df)\n",
    "    train_x = word_vectorizer.fit_transform(train_x)\n",
    "    test_x = word_vectorizer.transform(test_x)\n",
    "    print('Length of Vocabulary after vectorizing the corpus:',len(word_vectorizer.vocabulary_))\n",
    "    # Gridsearch to find the best values:\n",
    "    grid_search = GridSearchCV(estimator = model,\n",
    "                            param_grid = parameters, \n",
    "                            scoring = 'accuracy',\n",
    "                            cv = cv,\n",
    "                            n_jobs = -1, \n",
    "                            verbose = 2)\n",
    "    grid_search.fit(train_x,train_y)\n",
    "    print(f'Best Score: {grid_search.best_score_}. Best Params: {grid_search.best_params_}')\n",
    "    # It would better be to select \n",
    "\n",
    "    #Fitting the model for tfidf features\n",
    "    grid_search.best_estimator_.fit(train_x,train_y)\n",
    "    print(grid_search.best_estimator_)\n",
    "\n",
    "    #Predicting test\n",
    "    y_pred=grid_search.best_estimator_.predict(test_x)\n",
    "    print(y_pred)\n",
    "\n",
    "    #Predicting train\n",
    "    y_pred_train=grid_search.best_estimator_.predict(train_x)\n",
    "    print(y_pred_train)\n",
    "    ##Predicting the model for tfidf features\n",
    "\n",
    "    #Accuracy score test\n",
    "    print(\"Accuracy test:\",evaluate(test_y,y_pred)[0])\n",
    "    #Accuracy score train\n",
    "    print(\"Accuracy train:\",evaluate(train_y,y_pred_train)[0])\n",
    "    # Confiusionmatrix\n",
    "    print(\"Accuracy test:\",evaluate(test_y,y_pred)[1])\n",
    "    #Accuracy score train\n",
    "    print(\"Accuracy train:\",evaluate(train_y,y_pred_train)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Vocabulary after vectorizing the corpus: 6209089\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "Best Score: 0.500175. Best Params: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2'}\n",
      "LogisticRegression(C=0.01, max_iter=500)\n",
      "[0 0 0 ... 0 0 0]\n",
      "[1 1 1 ... 1 0 0]\n",
      "Accuracy test: 0.6061\n",
      "Accuracy train: 0.996275\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.56      0.98      0.71      4993\n",
      "    Positive       0.93      0.23      0.37      5007\n",
      "\n",
      "    accuracy                           0.61     10000\n",
      "   macro avg       0.75      0.61      0.54     10000\n",
      "weighted avg       0.75      0.61      0.54     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.99      1.00      1.00     20007\n",
      "    Positive       1.00      0.99      1.00     19993\n",
      "\n",
      "    accuracy                           1.00     40000\n",
      "   macro avg       1.00      1.00      1.00     40000\n",
      "weighted avg       1.00      1.00      1.00     40000\n",
      "\n",
      "Length of Vocabulary after vectorizing the corpus: 6209089\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "Best Score: 0.500175. Best Params: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2'}\n",
      "LogisticRegression(C=0.01, max_iter=500)\n",
      "[0 0 0 ... 0 1 1]\n",
      "[1 1 1 ... 1 0 0]\n",
      "Accuracy test: 0.715\n",
      "Accuracy train: 0.99625\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.66      0.89      0.76      4993\n",
      "    Positive       0.83      0.54      0.65      5007\n",
      "\n",
      "    accuracy                           0.71     10000\n",
      "   macro avg       0.75      0.72      0.71     10000\n",
      "weighted avg       0.75      0.71      0.71     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.99      1.00      1.00     20007\n",
      "    Positive       1.00      0.99      1.00     19993\n",
      "\n",
      "    accuracy                           1.00     40000\n",
      "   macro avg       1.00      1.00      1.00     40000\n",
      "weighted avg       1.00      1.00      1.00     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "lr=LogisticRegression()\n",
    "tv=TfidfVectorizer\n",
    "cv=CountVectorizer\n",
    "# Gridsearch to find the best values:\n",
    "parameters = [{'C': [0.01,0.1,1], 'penalty': ['l2'], 'max_iter':[500]}]\n",
    "# Count vectorizer\n",
    "vectorize_train_validate_evaluate(tv, n_gram=(1,3), analyzer='word', min_df=0, max_df=1, \n",
    "                                    model=lr, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)\n",
    "\n",
    "# Tfidf\n",
    "vectorize_train_validate_evaluate(cv, n_gram=(1,3), analyzer='word', min_df=0, max_df=1, \n",
    "                                    model=lr, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "142d007421900550079a12ae8655bcae678ebaad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Vocabulary after vectorizing the corpus: 17053\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "Best Score: 0.888525. Best Params: {'C': 1, 'max_iter': 500, 'penalty': 'l2'}\n",
      "LogisticRegression(C=1, max_iter=500)\n",
      "[0 0 0 ... 1 0 0]\n",
      "[1 1 1 ... 1 0 0]\n",
      "Accuracy test: 0.8986\n",
      "Accuracy train: 0.929425\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.90      0.90      0.90      4993\n",
      "    Positive       0.90      0.90      0.90      5007\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.94      0.92      0.93     20007\n",
      "    Positive       0.92      0.94      0.93     19993\n",
      "\n",
      "    accuracy                           0.93     40000\n",
      "   macro avg       0.93      0.93      0.93     40000\n",
      "weighted avg       0.93      0.93      0.93     40000\n",
      "\n",
      "Length of Vocabulary after vectorizing the corpus: 17053\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "Best Score: 0.884525. Best Params: {'C': 0.1, 'max_iter': 500, 'penalty': 'l2'}\n",
      "LogisticRegression(C=0.1, max_iter=500)\n",
      "[0 0 1 ... 0 0 0]\n",
      "[1 1 1 ... 1 0 0]\n",
      "Accuracy test: 0.8925\n",
      "Accuracy train: 0.964575\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.89      0.89      0.89      4993\n",
      "    Positive       0.89      0.89      0.89      5007\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.97      0.96      0.96     20007\n",
      "    Positive       0.96      0.97      0.96     19993\n",
      "\n",
      "    accuracy                           0.96     40000\n",
      "   macro avg       0.96      0.96      0.96     40000\n",
      "weighted avg       0.96      0.96      0.96     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "lr=LogisticRegression()\n",
    "tv=TfidfVectorizer\n",
    "cv=CountVectorizer\n",
    "# Gridsearch to find the best values:\n",
    "parameters = [{'C': [0.01,0.1,1], 'penalty': ['l2'], 'max_iter':[500]}]\n",
    "# Count vectorizer\n",
    "vectorize_train_validate_evaluate(tv, n_gram=(1,3), analyzer='word', min_df=0.001, max_df=0.999, \n",
    "                                    model=lr, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)\n",
    "\n",
    "# Tfidf\n",
    "vectorize_train_validate_evaluate(cv, n_gram=(1,3), analyzer='word', min_df=0.001, max_df=0.999, \n",
    "                                    model=lr, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chaning the ngram-range**\n",
    "\n",
    "As seen below, the training speeds up quickly when chaning the range of ngrams to 3. However, the accuracy on the test-set also drops from 89% to 67%, which is below our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Vocabulary after vectorizing the corpus: 789\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "Best Score: 0.6503. Best Params: {'C': 1, 'max_iter': 500, 'penalty': 'l2'}\n",
      "LogisticRegression(C=1, max_iter=500)\n",
      "[1 0 0 ... 1 0 0]\n",
      "[1 1 1 ... 1 0 0]\n",
      "Accuracy test: 0.6569\n",
      "Accuracy train: 0.670275\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.73      0.50      0.59      4993\n",
      "    Positive       0.62      0.81      0.70      5007\n",
      "\n",
      "    accuracy                           0.66     10000\n",
      "   macro avg       0.67      0.66      0.65     10000\n",
      "weighted avg       0.67      0.66      0.65     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.75      0.52      0.61     20007\n",
      "    Positive       0.63      0.82      0.71     19993\n",
      "\n",
      "    accuracy                           0.67     40000\n",
      "   macro avg       0.69      0.67      0.66     40000\n",
      "weighted avg       0.69      0.67      0.66     40000\n",
      "\n",
      "Length of Vocabulary after vectorizing the corpus: 789\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "Best Score: 0.6491750000000001. Best Params: {'C': 0.1, 'max_iter': 500, 'penalty': 'l2'}\n",
      "LogisticRegression(C=0.1, max_iter=500)\n",
      "[1 0 0 ... 1 0 0]\n",
      "[1 1 1 ... 1 0 0]\n",
      "Accuracy test: 0.6547\n",
      "Accuracy train: 0.666475\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.73      0.49      0.59      4993\n",
      "    Positive       0.62      0.82      0.70      5007\n",
      "\n",
      "    accuracy                           0.65     10000\n",
      "   macro avg       0.67      0.65      0.65     10000\n",
      "weighted avg       0.67      0.65      0.65     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.75      0.50      0.60     20007\n",
      "    Positive       0.63      0.83      0.71     19993\n",
      "\n",
      "    accuracy                           0.67     40000\n",
      "   macro avg       0.69      0.67      0.66     40000\n",
      "weighted avg       0.69      0.67      0.66     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "lr=LogisticRegression()\n",
    "tv=TfidfVectorizer\n",
    "cv=CountVectorizer\n",
    "# Gridsearch to find the best values:\n",
    "parameters = [{'C': [0.01,0.1,1], 'penalty': ['l2'], 'max_iter':[500]}]\n",
    "# Count vectorizer\n",
    "vectorize_train_validate_evaluate(tv, n_gram=(3,3), analyzer='word', min_df=0.001, max_df=0.999, \n",
    "                                    model=lr, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)\n",
    "\n",
    "# Tfidf\n",
    "vectorize_train_validate_evaluate(cv, n_gram=(3,3), analyzer='word', min_df=0.001, max_df=0.999, \n",
    "                                    model=lr, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using chars as ngram**\n",
    "\n",
    "We can see that the vocabulary-size drops from 17'000 to 7'900, even though we have the same n_gram range. The Quality on the test-set also drops by 2.7%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Support Vector Classifier**\n",
    "\n",
    "The Hyperparameter-tuning and Training is very quick. However, if we use all the words, it overfits heaviy, as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Vocabulary after vectorizing the corpus: 6209089\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "Best Score: 0.500175. Best Params: {'C': 1, 'max_iter': 500, 'penalty': 'l2'}\n",
      "LinearSVC(C=1, max_iter=500)\n",
      "[0 0 0 ... 0 1 1]\n",
      "[1 1 1 ... 1 0 0]\n",
      "Accuracy test: 0.7485\n",
      "Accuracy train: 0.996275\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.73      0.78      0.76      4993\n",
      "    Positive       0.77      0.72      0.74      5007\n",
      "\n",
      "    accuracy                           0.75     10000\n",
      "   macro avg       0.75      0.75      0.75     10000\n",
      "weighted avg       0.75      0.75      0.75     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.99      1.00      1.00     20007\n",
      "    Positive       1.00      0.99      1.00     19993\n",
      "\n",
      "    accuracy                           1.00     40000\n",
      "   macro avg       1.00      1.00      1.00     40000\n",
      "weighted avg       1.00      1.00      1.00     40000\n",
      "\n",
      "Length of Vocabulary after vectorizing the corpus: 6209089\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "Best Score: 0.500175. Best Params: {'C': 1, 'max_iter': 500, 'penalty': 'l2'}\n",
      "LinearSVC(C=1, max_iter=500)\n",
      "[0 0 0 ... 0 0 0]\n",
      "[1 1 1 ... 1 0 0]\n",
      "Accuracy test: 0.5091\n",
      "Accuracy train: 0.996275\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.50      1.00      0.67      4993\n",
      "    Positive       0.97      0.02      0.04      5007\n",
      "\n",
      "    accuracy                           0.51     10000\n",
      "   macro avg       0.74      0.51      0.35     10000\n",
      "weighted avg       0.74      0.51      0.35     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.99      1.00      1.00     20007\n",
      "    Positive       1.00      0.99      1.00     19993\n",
      "\n",
      "    accuracy                           1.00     40000\n",
      "   macro avg       1.00      1.00      1.00     40000\n",
      "weighted avg       1.00      1.00      1.00     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "svc=LinearSVC()\n",
    "tv=TfidfVectorizer\n",
    "cv=CountVectorizer\n",
    "# Gridsearch to find the best values. However, because of the long fitting time, we don't do a gridsearch if we use all the words (min_df = 1, max_df = 1.0)\n",
    "parameters = [{'C': [1, 0.1, 0.01], 'penalty': ['l2'], 'max_iter':[500]}]\n",
    "# Count vectorizer\n",
    "vectorize_train_validate_evaluate(tv, n_gram=(1,3), analyzer='word', min_df=0, max_df=1, \n",
    "                                    model=svc, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)\n",
    "\n",
    "# Tfidf\n",
    "vectorize_train_validate_evaluate(cv, n_gram=(1,3), analyzer='word', min_df=0, max_df=1, \n",
    "                                    model=svc, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Vocabulary after vectorizing the corpus: 17053\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "Best Score: 0.88965. Best Params: {'C': 0.1, 'max_iter': 500, 'penalty': 'l2'}\n",
      "LinearSVC(C=0.1, max_iter=500)\n",
      "[0 0 1 ... 1 0 0]\n",
      "[1 1 1 ... 1 0 0]\n",
      "Accuracy test: 0.899\n",
      "Accuracy train: 0.931325\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.90      0.89      0.90      4993\n",
      "    Positive       0.90      0.90      0.90      5007\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.94      0.92      0.93     20007\n",
      "    Positive       0.92      0.94      0.93     19993\n",
      "\n",
      "    accuracy                           0.93     40000\n",
      "   macro avg       0.93      0.93      0.93     40000\n",
      "weighted avg       0.93      0.93      0.93     40000\n",
      "\n",
      "Length of Vocabulary after vectorizing the corpus: 17053\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "Best Score: 0.8829. Best Params: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2'}\n",
      "LinearSVC(C=0.01, max_iter=500)\n",
      "[0 0 1 ... 0 0 0]\n",
      "[1 1 1 ... 1 0 0]\n",
      "Accuracy test: 0.8938\n",
      "Accuracy train: 0.967125\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.90      0.89      0.89      4993\n",
      "    Positive       0.89      0.90      0.89      5007\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.97      0.96      0.97     20007\n",
      "    Positive       0.96      0.97      0.97     19993\n",
      "\n",
      "    accuracy                           0.97     40000\n",
      "   macro avg       0.97      0.97      0.97     40000\n",
      "weighted avg       0.97      0.97      0.97     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "svc=LinearSVC()\n",
    "tv=TfidfVectorizer\n",
    "cv=CountVectorizer\n",
    "# Gridsearch to find the best values:\n",
    "parameters = [{'C': [1, 0.1, 0.01], 'penalty': ['l2'], 'max_iter':[500]}]\n",
    "# Count vectorizer\n",
    "vectorize_train_validate_evaluate(tv, n_gram=(1,3), analyzer='word', min_df=0.001, max_df=0.999, \n",
    "                                    model=svc, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)\n",
    "\n",
    "# Tfidf\n",
    "vectorize_train_validate_evaluate(cv, n_gram=(1,3), analyzer='word', min_df=0.001, max_df=0.999, \n",
    "                                    model=svc, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality, after filtering words out with min_df and max_df, is very good; it seems to avoid overfitting on a few, very specific tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Vocabulary after vectorizing the corpus: 153\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "Best Score: 0.7649. Best Params: {'C': 0.1, 'max_iter': 500, 'penalty': 'l2'}\n",
      "LinearSVC(C=0.1, max_iter=500)\n",
      "[0 0 0 ... 1 0 0]\n",
      "[1 1 1 ... 1 1 0]\n",
      "Accuracy test: 0.7608\n",
      "Accuracy train: 0.7685\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.76      0.75      0.76      4993\n",
      "    Positive       0.76      0.77      0.76      5007\n",
      "\n",
      "    accuracy                           0.76     10000\n",
      "   macro avg       0.76      0.76      0.76     10000\n",
      "weighted avg       0.76      0.76      0.76     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.77      0.76      0.77     20007\n",
      "    Positive       0.76      0.78      0.77     19993\n",
      "\n",
      "    accuracy                           0.77     40000\n",
      "   macro avg       0.77      0.77      0.77     40000\n",
      "weighted avg       0.77      0.77      0.77     40000\n",
      "\n",
      "Length of Vocabulary after vectorizing the corpus: 153\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "Best Score: 0.7638750000000001. Best Params: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2'}\n",
      "LinearSVC(C=0.01, max_iter=500)\n",
      "[0 0 0 ... 1 0 0]\n",
      "[1 1 1 ... 1 1 0]\n",
      "Accuracy test: 0.7625\n",
      "Accuracy train: 0.7678\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.77      0.75      0.76      4993\n",
      "    Positive       0.76      0.78      0.77      5007\n",
      "\n",
      "    accuracy                           0.76     10000\n",
      "   macro avg       0.76      0.76      0.76     10000\n",
      "weighted avg       0.76      0.76      0.76     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.78      0.75      0.76     20007\n",
      "    Positive       0.76      0.78      0.77     19993\n",
      "\n",
      "    accuracy                           0.77     40000\n",
      "   macro avg       0.77      0.77      0.77     40000\n",
      "weighted avg       0.77      0.77      0.77     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "svc=LinearSVC()\n",
    "tv=TfidfVectorizer\n",
    "cv=CountVectorizer\n",
    "# Gridsearch to find the best values:\n",
    "parameters = [{'C': [0.01,0.1,1], 'penalty': ['l2'], 'max_iter':[500]}]\n",
    "# Count vectorizer\n",
    "vectorize_train_validate_evaluate(tv, n_gram=(1,3), analyzer='word', min_df=0.1, max_df=0.9, \n",
    "                                    model=svc, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)\n",
    "\n",
    "# Tfidf\n",
    "vectorize_train_validate_evaluate(cv, n_gram=(1,3), analyzer='word', min_df=0.1, max_df=0.9, \n",
    "                                    model=svc, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that if we drop words, which appear in less than 10% of documents and those, which in more than 90% documents, we have a vocabulary size of 153. This leads to an awful test-accuracy of 76.6%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Vocabulary after vectorizing the corpus: 17108\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "Best Score: 0.889625. Best Params: {'C': 0.1, 'max_iter': 500, 'penalty': 'l2'}\n",
      "LinearSVC(C=0.1, max_iter=500)\n",
      "[0 0 1 ... 1 0 0]\n",
      "[1 1 1 ... 1 0 0]\n",
      "Accuracy test: 0.8991\n",
      "Accuracy train: 0.93145\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.90      0.89      0.90      4993\n",
      "    Positive       0.90      0.90      0.90      5007\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.94      0.92      0.93     20007\n",
      "    Positive       0.92      0.94      0.93     19993\n",
      "\n",
      "    accuracy                           0.93     40000\n",
      "   macro avg       0.93      0.93      0.93     40000\n",
      "weighted avg       0.93      0.93      0.93     40000\n",
      "\n",
      "Length of Vocabulary after vectorizing the corpus: 17108\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "Best Score: 0.8829. Best Params: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2'}\n",
      "LinearSVC(C=0.01, max_iter=500)\n",
      "[0 0 1 ... 0 0 0]\n",
      "[1 1 1 ... 1 0 0]\n",
      "Accuracy test: 0.8936\n",
      "Accuracy train: 0.966975\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.90      0.89      0.89      4993\n",
      "    Positive       0.89      0.90      0.89      5007\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.97      0.96      0.97     20007\n",
      "    Positive       0.96      0.97      0.97     19993\n",
      "\n",
      "    accuracy                           0.97     40000\n",
      "   macro avg       0.97      0.97      0.97     40000\n",
      "weighted avg       0.97      0.97      0.97     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "svc=LinearSVC()\n",
    "tv=TfidfVectorizer\n",
    "cv=CountVectorizer\n",
    "# Gridsearch to find the best values:\n",
    "parameters = [{'C': [0.01,0.1,1], 'penalty': ['l2'], 'max_iter':[500]}]\n",
    "# Count vectorizer\n",
    "vectorize_train_validate_evaluate(tv, n_gram=(1,4), analyzer='word', min_df=0.001, max_df=0.999, \n",
    "                                    model=svc, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)\n",
    "\n",
    "# Tfidf\n",
    "vectorize_train_validate_evaluate(cv, n_gram=(1,4), analyzer='word', min_df=0.001, max_df=0.999, \n",
    "                                    model=svc, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the ngram-range for the word-vectorizer increases the test-accuracy slightly to 89.91% and the length of the vocabulary to 17108."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Vocabulary after vectorizing the corpus: 7890\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "Best Score: 0.864325. Best Params: {'C': 1, 'max_iter': 500, 'penalty': 'l2'}\n",
      "LinearSVC(C=1, max_iter=500)\n",
      "[0 0 1 ... 1 1 0]\n",
      "[1 1 1 ... 1 0 0]\n",
      "Accuracy test: 0.8732\n",
      "Accuracy train: 0.9112\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.88      0.87      0.87      4993\n",
      "    Positive       0.87      0.88      0.87      5007\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.92      0.91      0.91     20007\n",
      "    Positive       0.91      0.92      0.91     19993\n",
      "\n",
      "    accuracy                           0.91     40000\n",
      "   macro avg       0.91      0.91      0.91     40000\n",
      "weighted avg       0.91      0.91      0.91     40000\n",
      "\n",
      "Length of Vocabulary after vectorizing the corpus: 7890\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vince\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.8468. Best Params: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vince\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=0.01, max_iter=500)\n",
      "[0 0 1 ... 1 1 0]\n",
      "[1 1 1 ... 1 0 0]\n",
      "Accuracy test: 0.8652\n",
      "Accuracy train: 0.921875\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.87      0.86      0.86      4993\n",
      "    Positive       0.86      0.87      0.87      5007\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.93      0.91      0.92     20007\n",
      "    Positive       0.92      0.93      0.92     19993\n",
      "\n",
      "    accuracy                           0.92     40000\n",
      "   macro avg       0.92      0.92      0.92     40000\n",
      "weighted avg       0.92      0.92      0.92     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "svc=LinearSVC()\n",
    "tv=TfidfVectorizer\n",
    "cv=CountVectorizer\n",
    "# Gridsearch to find the best values:\n",
    "parameters = [{'C': [0.01,0.1,1], 'penalty': ['l2'], 'max_iter':[500]}]\n",
    "# Count vectorizer\n",
    "vectorize_train_validate_evaluate(tv, n_gram=(1,3), analyzer='char', min_df=0.001, max_df=0.999, \n",
    "                                    model=svc, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)\n",
    "\n",
    "# Tfidf\n",
    "vectorize_train_validate_evaluate(cv, n_gram=(1,3), analyzer='char', min_df=0.001, max_df=0.999, \n",
    "                                    model=svc, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a char as analyzer, we get a vocabulary of 7890. However, this does not decrease the training-time though, because it does not converge in 500 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multinomial Naive Bayes for bag of words and tfidf features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Vocabulary after vectorizing the corpus: 6209089\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best Score: 0.5001749999999999. Best Params: {'alpha': 0.01}\n",
      "MultinomialNB(alpha=0.01)\n",
      "[0 0 0 ... 0 1 1]\n",
      "[1 1 1 ... 1 0 0]\n",
      "Accuracy test: 0.7512\n",
      "Accuracy train: 0.996275\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.75      0.75      0.75      4993\n",
      "    Positive       0.75      0.75      0.75      5007\n",
      "\n",
      "    accuracy                           0.75     10000\n",
      "   macro avg       0.75      0.75      0.75     10000\n",
      "weighted avg       0.75      0.75      0.75     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.99      1.00      1.00     20007\n",
      "    Positive       1.00      0.99      1.00     19993\n",
      "\n",
      "    accuracy                           1.00     40000\n",
      "   macro avg       1.00      1.00      1.00     40000\n",
      "weighted avg       1.00      1.00      1.00     40000\n",
      "\n",
      "Length of Vocabulary after vectorizing the corpus: 6983231\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best Score: 0.884125. Best Params: {'alpha': 1}\n",
      "MultinomialNB(alpha=1)\n",
      "[0 0 0 ... 0 1 0]\n",
      "[1 1 1 ... 1 0 0]\n",
      "Accuracy test: 0.8857\n",
      "Accuracy train: 0.99985\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.87      0.90      0.89      4993\n",
      "    Positive       0.90      0.87      0.88      5007\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      1.00      1.00     20007\n",
      "    Positive       1.00      1.00      1.00     19993\n",
      "\n",
      "    accuracy                           1.00     40000\n",
      "   macro avg       1.00      1.00      1.00     40000\n",
      "weighted avg       1.00      1.00      1.00     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "mnb=MultinomialNB()\n",
    "tv=TfidfVectorizer\n",
    "cv=CountVectorizer\n",
    "# Gridsearch to find the best values:\n",
    "parameters = [{'alpha': [0.01,0.1,1]}]\n",
    "# Count vectorizer\n",
    "vectorize_train_validate_evaluate(tv, n_gram=(1,3), analyzer='word', min_df=0, max_df=1, \n",
    "                                    model=mnb, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)\n",
    "\n",
    "# Tfidf\n",
    "vectorize_train_validate_evaluate(cv, n_gram=(1,3), analyzer='word', min_df=0, max_df=1., \n",
    "                                    model=mnb, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Vocabulary after vectorizing the corpus: 17053\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "Best Score: 0.87065. Best Params: {'alpha': 0.1}\n",
      "MultinomialNB(alpha=0.1)\n",
      "[0 0 0 ... 0 0 0]\n",
      "[1 1 1 ... 1 0 0]\n",
      "Accuracy test: 0.8727\n",
      "Accuracy train: 0.89095\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.88      0.86      0.87      4993\n",
      "    Positive       0.86      0.89      0.87      5007\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.91      0.87      0.89     20007\n",
      "    Positive       0.88      0.91      0.89     19993\n",
      "\n",
      "    accuracy                           0.89     40000\n",
      "   macro avg       0.89      0.89      0.89     40000\n",
      "weighted avg       0.89      0.89      0.89     40000\n",
      "\n",
      "Length of Vocabulary after vectorizing the corpus: 17053\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "Best Score: 0.8598250000000001. Best Params: {'alpha': 0.01}\n",
      "MultinomialNB(alpha=0.01)\n",
      "[0 0 0 ... 0 1 0]\n",
      "[1 1 1 ... 1 0 0]\n",
      "Accuracy test: 0.8642\n",
      "Accuracy train: 0.8757\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.87      0.86      0.86      4993\n",
      "    Positive       0.86      0.87      0.86      5007\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.88      0.87      0.87     20007\n",
      "    Positive       0.87      0.88      0.88     19993\n",
      "\n",
      "    accuracy                           0.88     40000\n",
      "   macro avg       0.88      0.88      0.88     40000\n",
      "weighted avg       0.88      0.88      0.88     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "mnb=MultinomialNB()\n",
    "tv=TfidfVectorizer\n",
    "cv=CountVectorizer\n",
    "# Gridsearch to find the best values:\n",
    "parameters = [{'alpha': [0.01,0.1,1]}]\n",
    "# Count vectorizer\n",
    "vectorize_train_validate_evaluate(tv, n_gram=(1,3), analyzer='word', min_df=0.001, max_df=0.999, \n",
    "                                    model=mnb, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)\n",
    "\n",
    "# Tfidf\n",
    "vectorize_train_validate_evaluate(cv, n_gram=(1,3), analyzer='word', min_df=0.001, max_df=0.999, \n",
    "                                    model=mnb, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Vocabulary after vectorizing the corpus: 7890\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "Best Score: 0.814025. Best Params: {'alpha': 1}\n",
      "MultinomialNB(alpha=1)\n",
      "[0 0 0 ... 1 1 0]\n",
      "[1 1 1 ... 1 0 0]\n",
      "Accuracy test: 0.8142\n",
      "Accuracy train: 0.82485\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.82      0.80      0.81      4993\n",
      "    Positive       0.81      0.83      0.82      5007\n",
      "\n",
      "    accuracy                           0.81     10000\n",
      "   macro avg       0.81      0.81      0.81     10000\n",
      "weighted avg       0.81      0.81      0.81     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.83      0.81      0.82     20007\n",
      "    Positive       0.82      0.84      0.83     19993\n",
      "\n",
      "    accuracy                           0.82     40000\n",
      "   macro avg       0.83      0.82      0.82     40000\n",
      "weighted avg       0.83      0.82      0.82     40000\n",
      "\n",
      "Length of Vocabulary after vectorizing the corpus: 7890\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "Best Score: 0.7776749999999999. Best Params: {'alpha': 0.01}\n",
      "MultinomialNB(alpha=0.01)\n",
      "[0 0 0 ... 1 1 0]\n",
      "[1 1 1 ... 1 0 0]\n",
      "Accuracy test: 0.7775\n",
      "Accuracy train: 0.783825\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.77      0.79      0.78      4993\n",
      "    Positive       0.79      0.76      0.77      5007\n",
      "\n",
      "    accuracy                           0.78     10000\n",
      "   macro avg       0.78      0.78      0.78     10000\n",
      "weighted avg       0.78      0.78      0.78     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.77      0.80      0.79     20007\n",
      "    Positive       0.79      0.77      0.78     19993\n",
      "\n",
      "    accuracy                           0.78     40000\n",
      "   macro avg       0.78      0.78      0.78     40000\n",
      "weighted avg       0.78      0.78      0.78     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "mnb=MultinomialNB()\n",
    "tv=TfidfVectorizer\n",
    "cv=CountVectorizer\n",
    "# Gridsearch to find the best values:\n",
    "parameters = [{'alpha': [0.01,0.1,1]}]\n",
    "# Count vectorizer\n",
    "vectorize_train_validate_evaluate(tv, n_gram=(1,3), analyzer='char', min_df=0.001, max_df=0.999, \n",
    "                                    model=mnb, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)\n",
    "\n",
    "# Tfidf\n",
    "vectorize_train_validate_evaluate(cv, n_gram=(1,3), analyzer='char', min_df=0.001, max_df=0.999, \n",
    "                                    model=mnb, parameters=parameters, train_x=norm_train.review, train_y=train_sentiments, \n",
    "                                    test_x=norm_test.review, test_y=test_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best Score: 0.8985. Best Params: {'C': 0.1, 'max_iter': 500, 'penalty': 'l2'}\n",
      "LinearSVC(C=0.1, max_iter=500)\n",
      "[0 0 1 ... 1 0 0]\n",
      "[1 1 1 ... 1 0 0]\n",
      "Accuracy test: 0.9013\n",
      "Accuracy train: 0.938875\n",
      "Accuracy test:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.91      0.89      0.90      4993\n",
      "    Positive       0.90      0.91      0.90      5007\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n",
      "Accuracy train:               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.95      0.93      0.94     20007\n",
      "    Positive       0.93      0.95      0.94     19993\n",
      "\n",
      "    accuracy                           0.94     40000\n",
      "   macro avg       0.94      0.94      0.94     40000\n",
      "weighted avg       0.94      0.94      0.94     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combination of word ngrams and char ngrams\n",
    "\"\"\"Pipeline to compare different types of preprocessing of words, models and parameters.\"\"\"\n",
    "# word ngrams\n",
    "word_vectorizer = TfidfVectorizer(ngram_range=(1,3), analyzer='word', min_df=0.001, max_df=0.999)\n",
    "train_word_x = word_vectorizer.fit_transform(norm_train.review)\n",
    "test_word_x = word_vectorizer.transform(norm_test.review)\n",
    "\n",
    "# char ngrams: \n",
    "word_vectorizer = TfidfVectorizer(ngram_range=(1,3), analyzer='char', min_df=0.001, max_df=0.999)\n",
    "train_char_x = word_vectorizer.fit_transform(norm_train.review)\n",
    "test_char_x = word_vectorizer.transform(norm_test.review)\n",
    "\n",
    "train_x, test_x = hstack([train_word_x, train_char_x]), hstack([test_word_x, test_char_x])\n",
    "\n",
    "del train_word_x, test_word_x, train_char_x, test_char_x\n",
    "# Gridsearch to find the best values:\n",
    "parameters = [{'C': [0.01,0.1,1], 'penalty': ['l2'], 'max_iter':[500]}]\n",
    "grid_search = GridSearchCV(estimator = LinearSVC(),\n",
    "                        param_grid = parameters, \n",
    "                        scoring = 'accuracy',\n",
    "                        cv = 5,\n",
    "                        n_jobs = -1, \n",
    "                        verbose = 2)\n",
    "grid_search.fit(train_x,train_sentiments)\n",
    "print(f'Best Score: {grid_search.best_score_}. Best Params: {grid_search.best_params_}')\n",
    "# It would better be to select \n",
    "\n",
    "#Fitting the model for tfidf features\n",
    "grid_search.best_estimator_.fit(train_x,train_sentiments)\n",
    "print(grid_search.best_estimator_)\n",
    "\n",
    "#Predicting test\n",
    "y_pred=grid_search.best_estimator_.predict(test_x)\n",
    "print(y_pred)\n",
    "\n",
    "#Predicting train\n",
    "y_pred_train=grid_search.best_estimator_.predict(train_x)\n",
    "print(y_pred_train)\n",
    "##Predicting the model for tfidf features\n",
    "\n",
    "#Accuracy score test\n",
    "print(\"Accuracy test:\",evaluate(test_sentiments,y_pred)[0])\n",
    "#Accuracy score train\n",
    "print(\"Accuracy train:\",evaluate(train_sentiments,y_pred_train)[0])\n",
    "# Confiusionmatrix\n",
    "print(\"Accuracy test:\",evaluate(test_sentiments,y_pred)[1])\n",
    "#Accuracy score train\n",
    "print(\"Accuracy train:\",evaluate(train_sentiments,y_pred_train)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data=pd.read_csv('data/IMDB Dataset.csv')\n",
    "_, test = train_test_split(imdb_data)\n",
    "wrong_results = test[test_sentiments != y_pred]\n",
    "correct_results = test[test_sentiments == y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_results.to_csv('wrong_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228.8915906788247"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_results.review.str.split().apply(len).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232.7214024187285"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_results.review.str.split().apply(len).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fehleranalyse**\n",
    "\n",
    "*Example 1:* \n",
    "\n",
    "Some of the reviews are even hard for us to decide if they are positive or negative. The following example could be seen as negative but is labelled as positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This movie is a remake of two movies that were a lot better. The last one, Heaven Can Wait, was great, I suggest you see that one. This one is not so great. The last third of the movie is not so bad and Chris Rock starts to show some of the comic fun that got him to where he is today. However, I don\\'t know what happened to the first two parts of this movie. It plays like some really bad \"B\" movie where people sound like they are in some bad TV sit-com. The situations are forced and it is like they are just trying to get the story over so they can start the real movie. It all seems real fake and the editing is just bad. I don\\'t know how they could release this movie like that. Anyway, the last part isn\\'t to bad, so wait for the video and see it then.']\n",
      "799    positive\n",
      "Name: sentiment, dtype: object\n"
     ]
    }
   ],
   "source": [
    "wrong_results = pd.read_csv('wrong_results.csv')\n",
    "sample = wrong_results.sample()\n",
    "print(sample.review.values)\n",
    "print(sample.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Example 2:* \n",
    "\n",
    "Other reviews are extremely long and contain details of the movie itself. What happens in the Movie is of course (mostly) irrelevant to decide if a review is positive or negative. Here, a transformer would probably help a lot; It could mark parts of the review which are important for the binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Go way back to page ten of this review section, and work your way back up. Go ahead; I\\'ll wait.<br /><br />Done? Well, then you\\'ve probably noticed the same trend that I have. You could nitpick all day long about the lame jokes, dated timing and obviously derivative plot points and shtick in \\'Bogus\\', but this movie seems to be one of those \\'hate me now, love me later\" flicks.<br /><br />Bill and Ted\\'s biggest problem was that the original appealed to those 80\\'s kids who followed the tends and considered themselves on the cutting edge of fashion. That worked fine for the original, but obviously bombed in 1991, when no self-unrespecting slacker would be caught dead wearing anything but flannel or admitting he liked anything about the 80\\'s. As Ted would put it: \"Dude, this is a totally deep hole. Wanna play 20 questions?\" <br /><br />They say nostalgia goes in 20 year cycles, and that certainly seems to be the case here. Here in 2010, those of use who grew up with Cindi Lauper and Megadeth are beginning to look back to appreciate some of those pivotal films that (like it or not) made us who we are. If you\\'re one of those who look at \\'Bogus\\' as if it were an outdated (\"Fa gs!\") ripoff, then you\\'re missing the point and probably spent six years growing facial hair, wearing flannel and looking like you just crawled out of bed.<br /><br />In self-referencing its own origins (the Star Trek episode\" and and time traveling with a phone booth), B&T makes no attempt to hide its creators\\' love for homage. Quentin Tarantino, anyone? I\\'m not saying \\'Bogus\\' is \\'True Love,\\' but I do think it needs to be appreciated for what it is: a fun snap-shot of our society at a time before child psychology, Ritalin, anti-smoking ads, terrorist paranoia and the proliferation of media fear-mongering. <br /><br />So, for all of you B&T haters out there...turn off your Screaming Trees CD, get that hair out of your face, go to the beach and lighten up. Narcissistic depression and intellectual ennui are SO corporate.']\n",
      "134    positive\n",
      "Name: sentiment, dtype: object\n"
     ]
    }
   ],
   "source": [
    "sample = wrong_results.sample()\n",
    "print(sample.review.values)\n",
    "print(sample.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Example 3:* \n",
    "\n",
    "The one below is again a review, which could be positive but is labelled as negative, especially when looking at this part of the review:\n",
    "\n",
    "> This film is subpar, though it delivers enough escapist entertainment and gratuitous nudity to please its intended audience (me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"A party-hardy frat boy's sister is brutally murdered by a street gang, sending the young man into a sudden psychotic rampage. He and his buddies massacre half the city to bring his sister back to life.<br /><br />SAVAGE STREETS was released a year after this film, and was more entertaining. Linnea Quigley, who has a costarring role in this film as the sexy (and briefly nude) girlfriend of one of the guys, also starred in SAVAGE STREETS.<br /><br />This film is subpar, though it delivers enough escapist entertainment and gratuitous nudity to please its intended audience (me).<br /><br />MPAA: Rated R for strong violence, nudity, language, and some sexuality.\"]\n",
      "83    negative\n",
      "Name: sentiment, dtype: object\n"
     ]
    }
   ],
   "source": [
    "sample = wrong_results.sample()\n",
    "print(sample.review.values)\n",
    "print(sample.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Example 4:*\n",
    "\n",
    " Following is an example, which is harder to explain. \n",
    "\n",
    "> I do not recommend this movie.\n",
    "\n",
    "This part should make the review clearly negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When a hardworking entrepreneur is rejected from a prestigious country club, he starts a battle between the members and eventually buys it from Ty Webb(Chevy Chase) and turns it into a theme park/golf course in which everyone can join.<br /><br />This is by no means a good movie, but it is still slightly amusing at times. Almost all of the comedy is cheap slapstick and bad jokes except for Chevy Chase. Chevy Chase plays one of his greatest roles as Ty Webb for a second time and plays it great. He is not only the funniest character in the movie, he is the only funny character in the movie. Even Dan Akroyd fails to bring humor to this film which aspires to be a great sequel to a classic comedy but falls to rubble with others shown on Comedy Central. The movie might have been better if Ty Webb(Chase) had a larger role but instead he was reduced to a minor character and the star became Jackie Mason(Who??) They should have brought back all of the cast and made a sequel the right way! This is a perfect example of what not to do when making a sequel to an already great movie. Overall, Caddyshack II is humorous but a large mistake.<br /><br />I do not recommend this movie.']\n",
      "718    negative\n",
      "Name: sentiment, dtype: object\n"
     ]
    }
   ],
   "source": [
    "sample = wrong_results.sample()\n",
    "print(sample.review.values)\n",
    "print(sample.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "407318ab8a71c1edb50740ee323cc7307e05568546678b79c25c054004b7234a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
