{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Sentiment Analysis\n",
    "\n",
    "This Jupyter Notebook illustrates the sentiment analysis of the IMDB dataset of the Kaggle competition https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews of over 50'000 movie reviews using deep learning. In order to run the notebook, the used packages like `troch` or `tensorflow` have to be installed first. Furthermore the word-embeddings have to be downloaded and saved into the appropriate folders. A more specific explanation of this can be found in the respective chapters.\n",
    "\n",
    "The founding author of this notebook is Tien Tran https://www.kaggle.com/tientd95, who has published his work on https://www.kaggle.com/tientd95/deep-learning-for-sentiment-analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"0.1\"></a>\n",
    "\n",
    "# **Table of Contents**\n",
    "\n",
    "\n",
    "1.\t[Processing Dataset](#1)\n",
    "2.  [Pretrained Word Embedding](#2)\n",
    "3.  [Building Model Pipeline](#3)\n",
    "4.  [Training Model with fastText Embedding](#4)\n",
    "5.  [Training Model with GloVe Embedding](#5)\n",
    "6.  [Model Evaluation](#6)\n",
    "7.  [Evaluation of FastText](#7)\n",
    "8.  [Interact with User's Input Review](#8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "import os, re, csv, math, codecs\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf  # we use both tensorflow and pytorch (pytorch for main part) , tensorflow for tokenizer\n",
    "\n",
    "from  utils import train_test_split, evaluate\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Processing Dataset** <a class=\"anchor\" id=\"1\"></a>\n",
    "\n",
    "The reviews are stored in the approx. 66.21 MB large 'IMDB Dataset.csv' in the directory 'data'. These have to be included first and can be downloaded from https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/IMDB Dataset.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The story is similar to ET: an extraterrestria...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To many people, Beat Street has inspired their...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Extremely entertaining mid-1950's western that...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  The story is similar to ET: an extraterrestria...          0\n",
       "1  To many people, Beat Street has inspired their...          1\n",
       "2  Extremely entertaining mid-1950's western that...          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert sentiment columns to numerical values\n",
    "df.sentiment = df.sentiment.apply(lambda x: 1 if x=='positive' else 0)\n",
    "\n",
    "X_train, X_test = train_test_split(df)\n",
    "\n",
    "# Random the rows of data\n",
    "X_train = X_train.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "# get label\n",
    "y_train = X_train.sentiment.values\n",
    "\n",
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check the distribution of the sentiments in all sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([20007, 19993]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(X_train.sentiment.values, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([4993, 5007]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(X_test.sentiment.values, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Pretrained Word Embedding** <a class=\"anchor\" id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fastText** is a word embedding development by Facebook released in 2016.\n",
    "fastText improves on Word2Vec by taking word parts into account, enables training of embeddings on smaller datasets and generalization to unknown words.\n",
    "\n",
    "The full version of fastText can be found here : https://fasttext.cc/docs/en/english-vectors.html \n",
    "\n",
    "Due to the size of memory, ( the full version is about 13GB RAM after loading), we use the mini version of fastText. This reduced embedding can be downloaded from https://fasttext.cc/docs/en/pretrained-vectors.html 'Simple English: bin+text, text' or directly from https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.simple.zip.\n",
    "\n",
    "The embeddings are all stored in the 'embeddings' directory. In this folder there is a subfolder for each embedding, if they contain more than one file. fastText embedding for example should be found under './embeddings/wiki.simple/wiki.simple.vec'.\n",
    "\n",
    "The vectors of this embedding have a length of 300. In the file these are organized in such a way that vectors of a length of 301 are stored. The first value in each case is the word in the vocabulary, whereas the 300 next elements represent the embedding vectors. Thus we extract the embedding into a dictionary `fasttext_embedding` with the key as the word and the value as the embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111052it [00:08, 13781.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load fasttext embeddings\n",
    "print('loading word embeddings...')\n",
    "fasttext_embedding = {}\n",
    "f = codecs.open('./embeddings/wiki.simple/wiki.simple.vec', encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    fasttext_embedding[word] = coefs\n",
    "f.close()\n",
    "\n",
    "# Check the dimensions of fastText\n",
    "fasttext_embedding['hello'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GloVe** (Global Vectors) is a word embedding which is developed as an open-source project at Stanford an was launched in 2014. The model is trained in an unsupervised manner for obtaining vector representations for words. It is based on co-occurence statistics from a corpus to map them into a semantical meaningful subspace.\n",
    "\n",
    "GloVe can be downloaded from https://www.kaggle.com/danielwillgeorge/glove6b100dtxt 'glove.6B.100d.txt'.\n",
    "\n",
    "The vectors of this embedding have a length of 100 and can be extracted very easly using dict-comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load GloVe embedding.\n",
    "glove = pd.read_csv('./embeddings/glove/glove.6B.100d.txt', sep=\" \", quoting=3, header=None, index_col=0)\n",
    "glove_embedding = {key: val.values for key, val in glove.T.items()}\n",
    "\n",
    "# Check the dimensions of GloVe\n",
    "glove_embedding['hello'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Building Model Pipeline** <a class=\"anchor\" id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset class\n",
    "\n",
    "First we need to create a Dataset class, take input in numpy array(embedding matrix) and return torch tensor output datatype "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset:\n",
    "    def __init__(self, reviews, targets):\n",
    "        \"\"\"\n",
    "        Argument:\n",
    "        reviews: a numpy array\n",
    "        targets: a vector array\n",
    "        \n",
    "        Return xtrain and ylabel in torch tensor datatype, stored in dictionary format\n",
    "        \"\"\"\n",
    "        self.reviews = reviews\n",
    "        self.target = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        # return length of dataset\n",
    "        return len(self.reviews)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # given an idex (item), return review and target of that index in torch tensor\n",
    "        review = torch.tensor(self.reviews[index,:], dtype = torch.long)\n",
    "        target = torch.tensor(self.target[index], dtype = torch.float)\n",
    "        \n",
    "        return {'review': review,\n",
    "                'target': target}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we move on to build a model class. Before that, there's something to remembe**r:\n",
    "\n",
    "* The input feed to model is served as embedding matrix (each row corresponding to an embedding vector of a word)\n",
    "* Number of words (for entire dataset) = number of row in embeddng matrix \n",
    "* Dimension of embedding is the num of columns in matrix, = dimention of pretrained embedding (fasttext, glove,..in case we use pretrained embedding). \n",
    "* Pretrained embeddings have several versions with different dimension so we should check the dimension before set dimension to model.\n",
    "* In case we use pretrainde embedding (this kernel), we will not do gadient calculation on these embedding (required grads = False)\n",
    "* In case we train embedding from scratch, we will treat embedding matrix as weight parameter and training on them (required grads = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embedding_matrix, flatten=False):\n",
    "        \"\"\"\n",
    "        Given embedding_matrix: numpy array with vector for all words\n",
    "        return prediction ( in torch tensor format)\n",
    "        \"\"\"\n",
    "        super(LSTM, self).__init__()\n",
    "        self.flatten = flatten\n",
    "        # Number of words = number of rows in embedding matrix\n",
    "        num_words = embedding_matrix.shape[0]\n",
    "        # Dimension of embedding is num of columns in the matrix\n",
    "        embedding_dim = embedding_matrix.shape[1]\n",
    "        # Define an input embedding layer\n",
    "        self.embedding = nn.Embedding(\n",
    "                                      num_embeddings=num_words,\n",
    "                                      embedding_dim=embedding_dim)\n",
    "        # Embedding matrix actually is collection of parameter\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype = torch.float32))\n",
    "        # Because we use pretrained embedding (GLove, Fastext,etc) so we turn off requires_grad-meaning we do not train gradient on embedding weight\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        # LSTM with hidden_size = 128\n",
    "        self.lstm = nn.LSTM(\n",
    "                            embedding_dim, \n",
    "                            128,\n",
    "                            bidirectional=True,\n",
    "                            batch_first=True,\n",
    "                             )\n",
    "        if flatten:\n",
    "            self.out = nn.Linear(128*256, 1)\n",
    "        else:\n",
    "            # Input(512) because we use bi-directional LSTM ==> hidden_size*2 + maxpooling **2  = 128*4 = 512, will be explained more on forward method\n",
    "            self.out = nn.Linear(512, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # pass input (tokens) through embedding layer\n",
    "        x = self.embedding(x)\n",
    "        # fit embedding to LSTM\n",
    "        hidden, _ = self.lstm(x)\n",
    "        if self.flatten:\n",
    "            flattened = torch.flatten(hidden, start_dim=1)\n",
    "            out = self.out(flattened)\n",
    "        else:\n",
    "            # apply mean and max pooling on lstm output\n",
    "            avg_pool= torch.mean(hidden, 1)\n",
    "            max_pool, index_max_pool = torch.max(hidden, 1)\n",
    "            # concat avg_pool and max_pool ( so we have 256 size, also because this is bidirectional ==> 256*2 = 512)\n",
    "            out = torch.cat((avg_pool, max_pool), 1)\n",
    "            # fit out to self.out to conduct dimensionality reduction to 1\n",
    "            out = self.out(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input of a forward pass is given by a matrix $X \\in \\mathbb{R}^{m \\times n}$. $m$ is the review dimension and $n$ is the word dimension. $X$ contains the indexes of the word embedding given by the word of the reviews. The reviews are defined into a fixed length using sequence padding, so that longer reviews are only considered up to `MAX_LEN` and shorter ones are padded with a special token. Thus, batching is greatly simplified.`torch.LSTM` then detects the padding automatically. Since `bidirectional` is True, the sequences are also passed through the LSTM from behind. So for a review we have a LSTM series of $2 \\times $ `MAX_LEN`. Afterwards the hidden state should contain all the information to perform the sentiment analysis. Because the hidden state is a matrix for a single sample, we need to reduce it's dimension. For this purpose we pursue two attempts:\n",
    "\n",
    "- First attempt is done by average, and max-pooling the hidden state of the LSTM. Afterwards we have left a vector representation of the sentiment analysis, which will be reduced with a linear layer to one dimension for the sentiment analysis. This was the attempt of the original author of this Notebook.\n",
    "- The second attempt is to flatten the output of the hidden state in order to feed it to a wider linear layer.\n",
    "\n",
    "The last linear layer in both versions have a single neuron as output. We then simply can take a threshold of 0.5 to split between a positive or negative review.\n",
    "\n",
    "Now after buidling the model class, we move to create `train` and `predict` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader, model, optimizer, device):\n",
    "    \"\"\"\n",
    "    this is model training for one epoch\n",
    "    data_loader:  this is torch dataloader, just like dataset but in torch and devide into batches\n",
    "    model : lstm\n",
    "    optimizer : torch optimizer : adam\n",
    "    device:  cuda or cpu\n",
    "    \"\"\"\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "    # go through batches of data in data loader\n",
    "    for data in data_loader:\n",
    "        reviews = data['review']\n",
    "        targets = data['target']\n",
    "        # move the data to device that we want to use\n",
    "        reviews = reviews.to(device, dtype = torch.long)\n",
    "        targets = targets.to(device, dtype = torch.float)\n",
    "        # clear the gradient\n",
    "        optimizer.zero_grad()\n",
    "        # make prediction from model\n",
    "        predictions = model(reviews)\n",
    "        # caculate the losses\n",
    "        loss = nn.BCEWithLogitsLoss()(predictions, targets.view(-1,1))\n",
    "        # backprob\n",
    "        loss.backward()\n",
    "        #single optimization step\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_loader, model, device):\n",
    "    final_predictions = []\n",
    "    final_targets = []\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            reviews = data['review']\n",
    "            targets = data['target']\n",
    "            reviews = reviews.to(device, dtype = torch.long)\n",
    "            targets = targets.to(device, dtype=torch.float)\n",
    "            # make prediction\n",
    "            predictions = model(reviews)\n",
    "            # move prediction and target to cpu\n",
    "            predictions = predictions.cpu().numpy().tolist()\n",
    "            targets = data['target'].cpu().numpy().tolist()\n",
    "            # add predictions to final_prediction\n",
    "            final_predictions.extend(predictions)\n",
    "            final_targets.extend(targets)\n",
    "    return final_predictions, final_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 8\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(word_index, embedding_dict=None, d_model=100):\n",
    "    \"\"\"\n",
    "     this function create the embedding matrix save in numpy array\n",
    "    :param word_index: a dictionary with word: index_value\n",
    "    :param embedding_dict: a dict with word embedding\n",
    "    :d_model: the dimension of word pretrained embedding, here I just set to 100, we will define again\n",
    "    :return a numpy array with embedding vectors for all known words\n",
    "    \"\"\"\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, d_model))\n",
    "    ## loop over all the words\n",
    "    for word, index in word_index.items():\n",
    "        if word in embedding_dict:\n",
    "            embedding_matrix[index] = embedding_dict[word]\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is time to running the model. \n",
    "The entire workflow will be as following steps:\n",
    "\n",
    "==> **Step1**: Creating a tokenizer function to convert sentences of dataset to token index\n",
    "After converting we'll have a dictionary contain word and its index. We feed it to creating an embedding matrix\n",
    "\n",
    "==> **Step2**: Cross validation of dataset to devide into train_df and valid_df\n",
    "\n",
    "==> **Step3**: Applying tokenizer pad_sequence to token index to ensure all sentence has the same vector dimension ( example: the sentence with 10 words will have longer vector dimension then the sentence with 2 words, using pad_sequence to ensure the same length, The length is set to a fixed number)\n",
    "\n",
    "We used the tokenizer from `tensorflow` 2 because of its convenience in pad_sequence.\n",
    "\n",
    "==> **Step4**: Initialize dataset class `IMDBDataset`\n",
    "\n",
    "==> **Step5**: We load the dataset which created in step4 to Pytorch DataLoader in order to devide the dataset to batches\n",
    "\n",
    "==> **Step6**: Till now, we have almost necessary components to start training. Calling model, optimizer, send model to device and start running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Training Model with fastText Embedding** <a class=\"anchor\" id=\"4\"></a>\n",
    "\n",
    "\n",
    "fastText embedding version in this kernel is 300, so we set d_model =300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Tokenization \n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(df.review.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load fasttext embedding\n",
      "training model\n",
      "epoch: 0, accuracy_train: 0.858125,    accuracy_test: 0.8409\n",
      "epoch: 1, accuracy_train: 0.89355,    accuracy_test: 0.8577\n",
      "epoch: 2, accuracy_train: 0.91785,    accuracy_test: 0.8608\n",
      "epoch: 3, accuracy_train: 0.939175,    accuracy_test: 0.8643\n",
      "epoch: 4, accuracy_train: 0.937525,    accuracy_test: 0.8543\n"
     ]
    }
   ],
   "source": [
    "print('Load fasttext embedding')\n",
    "embedding_matrix = create_embedding_matrix(tokenizer.word_index, embedding_dict=fasttext_embedding, d_model=300)\n",
    "\n",
    "# pad sequence\n",
    "xtrain = tokenizer.texts_to_sequences(X_train.review.values)\n",
    "xtest = tokenizer.texts_to_sequences(X_test.review.values)\n",
    "\n",
    "# zero padding\n",
    "xtrain = tf.keras.preprocessing.sequence.pad_sequences(xtrain, maxlen=MAX_LEN)\n",
    "xtest = tf.keras.preprocessing.sequence.pad_sequences(xtest, maxlen=MAX_LEN)\n",
    "\n",
    "# initialize dataset class for training\n",
    "train_dataset = IMDBDataset(reviews=xtrain, targets=X_train.sentiment.values)\n",
    "\n",
    "# Load dataset to Pytorch DataLoader\n",
    "# after we have train_dataset, we create a torch dataloader to load train_dataset class based on specified batch_size\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size = TRAIN_BATCH_SIZE, num_workers=2)\n",
    "# initialize dataset class for validation\n",
    "valid_dataset = IMDBDataset(reviews=xtest, targets=X_test.sentiment.values)\n",
    "valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = VALID_BATCH_SIZE, num_workers=1)\n",
    "\n",
    "# Running \n",
    "device = torch.device('cuda')\n",
    "# feed embedding matrix to lstm\n",
    "model_fasttext = LSTM(embedding_matrix, flatten=False)\n",
    "# set model to cuda device\n",
    "model_fasttext.to(device)\n",
    "# initialize Adam optimizer\n",
    "optimizer = torch.optim.Adam(model_fasttext.parameters(), lr=1e-3)\n",
    "\n",
    "print('training model')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    #train one epoch\n",
    "    train(train_data_loader, model_fasttext, optimizer, device)\n",
    "    #validate\n",
    "    outputs_fasttext_train, targets_fasttext_train = predict(train_data_loader, model_fasttext, device)\n",
    "    outputs_fasttext, targets_fasttext = predict(valid_data_loader, model_fasttext, device)\n",
    "    # threshold\n",
    "    outputs_fasttext_train = np.array(outputs_fasttext_train) >= 0.5\n",
    "    outputs_fasttext = np.array(outputs_fasttext) >= 0.5\n",
    "    # calculate accuracy\n",
    "    accuracy_train = metrics.accuracy_score(targets_fasttext_train, outputs_fasttext_train)\n",
    "    accuracy = metrics.accuracy_score(targets_fasttext, outputs_fasttext)\n",
    "    print(f'epoch: {epoch}, accuracy_train: {accuracy_train},    accuracy_test: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we flatten the output of the hidden state instead of pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load fasttext embedding\n",
      "training model flatten\n",
      "epoch: 0, accuracy_train: 0.83725,    accuracy_test: 0.8123\n",
      "epoch: 1, accuracy_train: 0.88265,    accuracy_test: 0.8072\n",
      "epoch: 2, accuracy_train: 0.95245,    accuracy_test: 0.8121\n",
      "epoch: 3, accuracy_train: 0.976725,    accuracy_test: 0.8133\n",
      "epoch: 4, accuracy_train: 0.912525,    accuracy_test: 0.7554\n"
     ]
    }
   ],
   "source": [
    "print('Load fasttext embedding')\n",
    "embedding_matrix = create_embedding_matrix(tokenizer.word_index, embedding_dict=fasttext_embedding, d_model=300)\n",
    "\n",
    "# pad sequence\n",
    "xtrain = tokenizer.texts_to_sequences(X_train.review.values)\n",
    "xtest = tokenizer.texts_to_sequences(X_test.review.values)\n",
    "\n",
    "# zero padding\n",
    "xtrain = tf.keras.preprocessing.sequence.pad_sequences(xtrain, maxlen=MAX_LEN)\n",
    "xtest = tf.keras.preprocessing.sequence.pad_sequences(xtest, maxlen=MAX_LEN)\n",
    "\n",
    "# initialize dataset class for training\n",
    "train_dataset = IMDBDataset(reviews=xtrain, targets=X_train.sentiment.values)\n",
    "\n",
    "# Load dataset to Pytorch DataLoader\n",
    "# after we have train_dataset, we create a torch dataloader to load train_dataset class based on specified batch_size\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size = TRAIN_BATCH_SIZE, num_workers=2)\n",
    "# initialize dataset class for validation\n",
    "valid_dataset = IMDBDataset(reviews=xtest, targets=X_test.sentiment.values)\n",
    "valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = VALID_BATCH_SIZE, num_workers=1)\n",
    "\n",
    "# Running \n",
    "device = torch.device('cuda')\n",
    "# feed embedding matrix to lstm\n",
    "model_fasttext_flatten = LSTM(embedding_matrix, flatten=True)\n",
    "# set model to cuda device\n",
    "model_fasttext_flatten.to(device)\n",
    "# initialize Adam optimizer\n",
    "optimizer = torch.optim.Adam(model_fasttext_flatten.parameters(), lr=1e-3)\n",
    "\n",
    "print('training model flatten')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    #train one epoch\n",
    "    train(train_data_loader, model_fasttext_flatten, optimizer, device)\n",
    "    #validate\n",
    "    outputs_fasttext_flatten_train, targets_fasttext_flatten_train = predict(train_data_loader,\n",
    "                                                                             model_fasttext_flatten, device)\n",
    "    outputs_fasttext_flatten, targets_fasttext_flatten = predict(valid_data_loader, model_fasttext_flatten, device)\n",
    "    # threshold\n",
    "    outputs_fasttext_flatten_train = np.array(outputs_fasttext_flatten_train) >= 0.5\n",
    "    outputs_fasttext_flatten = np.array(outputs_fasttext_flatten) >= 0.5\n",
    "    # calculate accuracy\n",
    "    accuracy_train = metrics.accuracy_score(targets_fasttext_flatten_train, outputs_fasttext_flatten_train)\n",
    "    accuracy = metrics.accuracy_score(targets_fasttext_flatten, outputs_fasttext_flatten)\n",
    "    print(f'epoch: {epoch}, accuracy_train: {accuracy_train},    accuracy_test: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Training Model with GloVe Embedding** <a class=\"anchor\" id=\"5\"></a>\n",
    "\n",
    "GloVe embedding version in this kernel is 100, so we set d_model =100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Glove embedding\n",
      "training model\n",
      "epoch: 0, accuracy_train: 0.8327,    accuracy_test: 0.8257\n",
      "epoch: 1, accuracy_train: 0.863675,    accuracy_test: 0.8456\n",
      "epoch: 2, accuracy_train: 0.87955,    accuracy_test: 0.8515\n",
      "epoch: 3, accuracy_train: 0.89585,    accuracy_test: 0.8538\n",
      "epoch: 4, accuracy_train: 0.90885,    accuracy_test: 0.8544\n"
     ]
    }
   ],
   "source": [
    "print('Load Glove embedding')\n",
    "embedding_matrix = create_embedding_matrix(tokenizer.word_index, embedding_dict=glove_embedding, d_model=100)\n",
    "\n",
    "# pad sequence\n",
    "xtrain = tokenizer.texts_to_sequences(X_train.review.values)\n",
    "xtest = tokenizer.texts_to_sequences(X_test.review.values)\n",
    "\n",
    "# zero padding\n",
    "xtrain = tf.keras.preprocessing.sequence.pad_sequences(xtrain, maxlen=MAX_LEN)\n",
    "xtest = tf.keras.preprocessing.sequence.pad_sequences(xtest, maxlen=MAX_LEN)\n",
    "\n",
    "# initialize dataset class for training\n",
    "train_dataset = IMDBDataset(reviews=xtrain, targets=X_train.sentiment.values)\n",
    "\n",
    "# load dataset to Pytorch DataLoader\n",
    "# after we have train_dataset, we create a torch dataloader to load train_dataset class based on specified batch_size\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size = TRAIN_BATCH_SIZE, num_workers=2)\n",
    "# initialize dataset class for validation\n",
    "valid_dataset = IMDBDataset(reviews=xtest, targets=X_test.sentiment.values)\n",
    "valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = VALID_BATCH_SIZE, num_workers=1)\n",
    "\n",
    "# Running \n",
    "device = torch.device('cuda')\n",
    "# feed embedding matrix to lstm\n",
    "model_glove = LSTM(embedding_matrix, flatten=False)\n",
    "# set model to cuda device\n",
    "model_glove.to(device)\n",
    "# initialize Adam optimizer\n",
    "optimizer = torch.optim.Adam(model_glove.parameters(), lr=1e-3)\n",
    "\n",
    "print('training model')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    #train one epoch\n",
    "    train(train_data_loader, model_glove, optimizer, device)\n",
    "    #validate\n",
    "    outputs_glove_train, targets_glove_train = predict(train_data_loader, model_glove, device)\n",
    "    outputs_glove, targets_glove = predict(valid_data_loader, model_glove, device)\n",
    "    # threshold\n",
    "    outputs_glove_train = np.array(outputs_glove_train) >= 0.5\n",
    "    outputs_glove = np.array(outputs_glove) >= 0.5\n",
    "    # calculate accuracy\n",
    "    accuracy_train = metrics.accuracy_score(targets_glove_train, outputs_glove_train)\n",
    "    accuracy = metrics.accuracy_score(targets_glove, outputs_glove)\n",
    "    print(f'epoch: {epoch}, accuracy_train: {accuracy_train},    accuracy_test: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we flatten the output of the hidden state instead of pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Glove embedding\n",
      "training model flatten\n",
      "epoch: 0, accuracy_train: 0.828025,    accuracy_test: 0.8085\n",
      "epoch: 1, accuracy_train: 0.879025,    accuracy_test: 0.8134\n",
      "epoch: 2, accuracy_train: 0.949075,    accuracy_test: 0.8276\n",
      "epoch: 3, accuracy_train: 0.967975,    accuracy_test: 0.8268\n",
      "epoch: 4, accuracy_train: 0.956125,    accuracy_test: 0.8051\n"
     ]
    }
   ],
   "source": [
    "print('Load Glove embedding')\n",
    "embedding_matrix = create_embedding_matrix(tokenizer.word_index, embedding_dict=glove_embedding, d_model=100)\n",
    "\n",
    "# pad sequence\n",
    "xtrain = tokenizer.texts_to_sequences(X_train.review.values)\n",
    "xtest = tokenizer.texts_to_sequences(X_test.review.values)\n",
    "\n",
    "# zero padding\n",
    "xtrain = tf.keras.preprocessing.sequence.pad_sequences(xtrain, maxlen=MAX_LEN)\n",
    "xtest = tf.keras.preprocessing.sequence.pad_sequences(xtest, maxlen=MAX_LEN)\n",
    "\n",
    "# initialize dataset class for training\n",
    "train_dataset = IMDBDataset(reviews=xtrain, targets=X_train.sentiment.values)\n",
    "\n",
    "# load dataset to Pytorch DataLoader\n",
    "# after we have train_dataset, we create a torch dataloader to load train_dataset class based on specified batch_size\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size = TRAIN_BATCH_SIZE, num_workers=2)\n",
    "# initialize dataset class for validation\n",
    "valid_dataset = IMDBDataset(reviews=xtest, targets=X_test.sentiment.values)\n",
    "valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = VALID_BATCH_SIZE, num_workers=1)\n",
    "\n",
    "# Running \n",
    "device = torch.device('cuda')\n",
    "# feed embedding matrix to lstm\n",
    "model_glove_flatten = LSTM(embedding_matrix, flatten=True)\n",
    "# set model to cuda device\n",
    "model_glove_flatten.to(device)\n",
    "# initialize Adam optimizer\n",
    "optimizer = torch.optim.Adam(model_glove_flatten.parameters(), lr=1e-3)\n",
    "\n",
    "print('training model flatten')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    #train one epoch\n",
    "    train(train_data_loader, model_glove_flatten, optimizer, device)\n",
    "    #validate\n",
    "    outputs_glove_flatten_train, targets_glove_flatten_train = predict(train_data_loader,\n",
    "                                                                       model_glove_flatten, device)\n",
    "    outputs_glove_flatten, targets_glove_flatten = predict(valid_data_loader, model_glove_flatten, device)\n",
    "    # threshold\n",
    "    outputs_glove_flatten_train = np.array(outputs_glove_flatten_train) >= 0.5\n",
    "    outputs_glove_flatten = np.array(outputs_glove_flatten) >= 0.5\n",
    "    # calculate accuracy\n",
    "    accuracy_train = metrics.accuracy_score(targets_glove_flatten_train, outputs_glove_flatten_train)\n",
    "    accuracy = metrics.accuracy_score(targets_glove_flatten, outputs_glove_flatten)\n",
    "    print(f'epoch: {epoch}, accuracy_train: {accuracy_train},    accuracy_test: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6. Model Evaluation** <a class=\"anchor\" id=\"6\"></a>\n",
    "\n",
    "In this section we're looking at the evaluation of our four models. The models where the hidden state was flattened instead of pooled have the ending `_flatten`.\n",
    "\n",
    "## fastText\n",
    "\n",
    "**Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8543\n"
     ]
    }
   ],
   "source": [
    "evaluation_fasttext = evaluate(targets_fasttext, outputs_fasttext)\n",
    "print(evaluation_fasttext[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7554\n"
     ]
    }
   ],
   "source": [
    "evaluation_fasttext_flatten = evaluate(targets_fasttext_flatten, outputs_fasttext_flatten)\n",
    "print(evaluation_fasttext_flatten[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**classification-report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.80      0.94      0.87      4993\n",
      "    Positive       0.93      0.77      0.84      5007\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.86      0.85      0.85     10000\n",
      "weighted avg       0.87      0.85      0.85     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(evaluation_fasttext[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.68      0.95      0.79      4993\n",
      "    Positive       0.91      0.57      0.70      5007\n",
      "\n",
      "    accuracy                           0.76     10000\n",
      "   macro avg       0.80      0.76      0.75     10000\n",
      "weighted avg       0.80      0.76      0.75     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(evaluation_fasttext_flatten[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe\n",
    "\n",
    "**Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8544\n"
     ]
    }
   ],
   "source": [
    "evaluation_glove = evaluate(targets_glove, outputs_glove)\n",
    "print(evaluation_glove[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8051\n"
     ]
    }
   ],
   "source": [
    "evaluation_glove_flatten = evaluate(targets_glove_flatten, outputs_glove_flatten)\n",
    "print(evaluation_glove_flatten[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**classification-report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.79      0.96      0.87      4993\n",
      "    Positive       0.95      0.75      0.84      5007\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.87      0.85      0.85     10000\n",
      "weighted avg       0.87      0.85      0.85     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(evaluation_glove[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.75      0.92      0.83      4993\n",
      "    Positive       0.90      0.69      0.78      5007\n",
      "\n",
      "    accuracy                           0.81     10000\n",
      "   macro avg       0.82      0.81      0.80     10000\n",
      "weighted avg       0.82      0.81      0.80     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(evaluation_glove_flatten[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "The best results were achieved with an **accuracy** of **0.85** on the test set with the current setting. This however is pretty arbitrary since with just another seed of the shuffling of the training set (after splitting) both best models achieved **accuracy** of **0.88**. So that means that the shuffling of the training set led to find another local minima of the model.\n",
    "\n",
    "The results show very clearly that the flattened version of the LSTM's hidden state does perform worse than the pooling version. If we compare the accuracy of the train- and the test set it's pretty obvious that the flattened models overfits. We suspect it is because the linear layer of the flattened version has much more weights rather than just pooling. So we think that the pooling version is better in generalization because pooling in itself has regularizational properties. Regularization of the flattened versions would have been a vaild attempt to improve performance.\n",
    "\n",
    "Most test have shown that **FastText** and **GloVe** performed pretty similar in accuracy, precision and recall on the test-set. So the performance does not really depend on the Word-Embedding in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **7. Evaluation of FastText** <a class=\"anchor\" id=\"7\"></a>\n",
    "\n",
    "Here we look at a few examples of incorrectly predicted reviews from the **FastText** model. In particular, we are interested in the **FP** and **FN** predictions. First, we need to extract them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4693,  300],\n",
       "       [1157, 3850]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_sample(model, sentence):\n",
    "    model.eval()\n",
    "    sentence_original = sentence\n",
    "    sentence = np.array([sentence])\n",
    "    sentence_token = tokenizer.texts_to_sequences(sentence)\n",
    "    sentence_token = tf.keras.preprocessing.sequence.pad_sequences(sentence_token, maxlen = MAX_LEN)\n",
    "    sentence_train = torch.tensor(sentence_token, dtype = torch.long).to(device, dtype = torch.long)\n",
    "    predict = model(sentence_train)\n",
    "    if predict.item() > 0.5:\n",
    "        print('------> Positive')\n",
    "    else:\n",
    "        print('------> Negative')\n",
    "    print(sentence_original)\n",
    "\n",
    "fasttext_evaluation = targets_fasttext - outputs_fasttext[:,0]\n",
    "fasttext_fp_idxs = np.where(fasttext_evaluation == -1)[0]\n",
    "fasttext_fn_idxs = np.where(fasttext_evaluation == 1)[0]\n",
    "confusion_matrix(targets_fasttext, outputs_fasttext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relatively high **TP** and **TN** predictions was to be expected given the relatively high accuracy of the model and the balance of the data set. In the current setting however it is slightly biased to predict the positive reviews more acurate than the negativ reviews. According to the **FP** and **FN** it seems that the model makes by far more **FN** than **FP**. However, the distribution of all four values is quiet arbitrary since we achieved pretty balanced **TP** to **TN** and **FN** to **FP** with a different random seed.\n",
    "\n",
    "Now we look at three examples of the wrong predictions. *The \"positive\" or \"negative\" is the prediction of the model, not the label. Keep in mind!*\n",
    "\n",
    "**FP** examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------> Positive\n",
      "Still a sucker for Pyun's esthetic sense, I liked this movie, though the \"unfinished\" ending was a let-down. As usual, Pyun develops a warped sense of humour and Kathy Long's fights are extremely impressive. Beautifully photographed, this has the feel it was done for the big screen.\n"
     ]
    }
   ],
   "source": [
    "evaluate_sample(model_fasttext, X_test.iloc[fasttext_fp_idxs[2]]['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------> Positive\n",
      "\"Black Vengeance\" is an alternate title for \"Ying hung ho hon\" AKA \"Tragic Hero\" (1987). I have just seen this on VHS, together with the first part of the story, \"Gong woo ching\" (\"Rich and Famous\"), also 1987. (The poster and 2 stills featured on the page are for a 4-DVD set of movies starring Rod Perry (The Black Gestapo), Fred Williamson (Black Cobra 2), Richard Lawson (Black Fist). The fourth movie is called \"The Black Six\"). Strangely, while the characters retain their original names in \"Rich and Famous\", in \"Black Vengeance\" Chow Yun-Fat's character is named Eddie Shaw, Alex Man (Man Tze Leung) is Harry, and Andy Lau is called Johnny. Also confusing is the fact that 1994 is given as the copyright dates on both films. Perhaps that was the year they were American-dubbed. According to the release dates given on IMDb \"Tragic Hero\" was released before \"Rich and Famous\". Was there any reason for releasing the sequel first? Despite some users' comments, I enjoyed these films, although they aren't among CYF's best such as \"The Killer\" and \"Hard-Boiled\" which are truly astonishing. However,if one day I come across a 2-DVD set of \"Rich and Famous\" and \"Tragic Hero\" I won't hesitate to buy it. Hopefully, these comments about \"Black Vengeance\" clear up, which was also for me, a mystery as to where it belonged in Chow Yun-Fat's filmography.\n"
     ]
    }
   ],
   "source": [
    "evaluate_sample(model_fasttext, X_test.iloc[fasttext_fp_idxs[3]]['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------> Positive\n",
      "I watched this movie when I was a young lad full of raging hormones and it was about as sexy a movie as I had ever seen-or ever was to see. It may not have been a great movie. My guess is it wasn't. I don't really remember much about it, to tell you the truth. I only remember the sexual chemistry between Crosby and Biehn. No woman in ANY movie has ever done it for me as the unbelievably sexy Cathy did in this movie. I haven't seen it since that first time I caught it on TV in the 70s and I don't think I'd want to see it again since I'm sure it would be a disappointment-my hormones aren't as raging and I've become more jaded over the years. Still, when I think back on the shower scene I can still remember how great it felt way back when.<br /><br />Added later: After watching the movie again, I discovered that it's dangerous to go home again. What was once erotic is now pretty tame. The older woman-younger man thing still works for me, just not as much as it once did, probably because I'm no longer a 12-year-old. That older woman is now younger than I am. Also, the amateurishness of the whole thing wasn't perceived by my twelve-year-old mind. <br /><br />Moral: Sometimes it's better not to revisit the past.\n"
     ]
    }
   ],
   "source": [
    "evaluate_sample(model_fasttext, X_test.iloc[fasttext_fp_idxs[4]]['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the examples above it can be pretty difficult to classify these reviews correctly because they are not as extremly negative as the usual reviews. They have some positive sentences in it. And if you e.g. look at the 2nd example, to me it looks like this is an actual positive review and it was just wrongly labelled.\n",
    "\n",
    "**FN** examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------> Negative\n",
      "I actually had seen the last parts of this movie when I was a child. Thanks to the search feature of plots I was able to find out the name of it. For years I did not know the name, but the movie stuck in my mind. The ending left hope that the main character would get back to Earth eventually. It was a shame it did not make it to a series. This movie reminds me of Journey to The Far Side Of the Sun. Also known as Doppleganger. If you liked this feature the other one is worth a watch. It was done before The Stranger, but shares a similar plot. Yet different. I just picked up The Stranger off of eBay on VHS. Hope they make a DVD, but it is doubtful unless it comes out on Dollar DVD. A few pilots are making it on the budget DVD's and maybe this one will.\n"
     ]
    }
   ],
   "source": [
    "evaluate_sample(model_fasttext, X_test.iloc[fasttext_fn_idxs[2]]['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------> Negative\n",
      "This is a very dark movie, somewhat better than the average Asylum film. It was a lot better than I thought it would be, is a combination of a psychological thriller and a horror film. <br /><br />The voice on the telephone is really creepy - this voice without a face, this unknown and threatening voice works really well in the film, since we never see the killer face is left to the imagination of the spectator.<br /><br />The action and suspense never decay and after the first half of the film, it becomes vertiginous; there is not much gore in this film, just enough to serve the story and also the director does a good job at holding your attention. <br /><br />I gave this movie a 8/10 because some clichés.\n"
     ]
    }
   ],
   "source": [
    "evaluate_sample(model_fasttext, X_test.iloc[fasttext_fn_idxs[3]]['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------> Negative\n",
      "The good thing about this film is that it stands alone - you don't have to have seen the original. Unfortunately this is also it's biggest drawback. It would have been nice to have included a few of the original characters in the new story and seen how their lives had developed. Sinclair as in the original is excellent and provides the films best comic moments as he attempts to deal with awkward and embarrassing situations but the supporting cast is not as strong as in the original movie. Forsyth is to be congratulated on a brave attempt to move the character on and create an original sequel but the film is ultimately flawed and lacks the warmth of the original\n"
     ]
    }
   ],
   "source": [
    "evaluate_sample(model_fasttext, X_test.iloc[fasttext_fn_idxs[4]]['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here again, these reviews are not very clearly positive or negative. For instance the 3rd one I can not really tell if the writer meant to write it as a positive or negative review. The only clearly missclassified review is the 2nd one in my opinion. But it is still pretty hard to tell if you don't read the last sentence of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **8. Interact with User's Input Review** <a class=\"anchor\" id=\"8\"></a>\n",
    "\n",
    "Now it's time to test model by entering any review we can think and see the model reaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Interact_user_input(model):\n",
    "    '''\n",
    "    model: trained model : fasttext model or glove model\n",
    "    '''\n",
    "    model.eval()\n",
    "    \n",
    "    sentence = ''\n",
    "    while True:\n",
    "        try:\n",
    "            sentence = input('Review: ')\n",
    "            if sentence in ['q','quit']: \n",
    "                break\n",
    "            sentence = np.array([sentence])\n",
    "            sentence_token = tokenizer.texts_to_sequences(sentence)\n",
    "            sentence_token = tf.keras.preprocessing.sequence.pad_sequences(sentence_token, maxlen = MAX_LEN)\n",
    "            sentence_train = torch.tensor(sentence_token, dtype = torch.long).to(device, dtype = torch.long)\n",
    "            predict = model(sentence_train)\n",
    "            if predict.item() > 0.5:\n",
    "                print('------> Positive')\n",
    "            else:\n",
    "                print('------> Negative')\n",
    "        except KeyError:\n",
    "            print('please enter again')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter reviews and test the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Interact_user_input(model_fasttext)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "npr",
   "language": "python",
   "name": "npr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
